{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0efb103-1522-4574-8f2c-d7636cfdc868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d95fe911-7d59-40d8-934a-1f2cb8d131f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "#observation, info = env.reset()\n",
    "\n",
    "def q_learn(env, episodes):\n",
    "    alpha = 0.1   # learning rate\n",
    "    gamma = 0.99  # discount factor, importance of future rewards\n",
    "    epsilon = 0.1 # how greedy the model should be \n",
    "\n",
    "    num_states = env.observation_space.shape[0]  # the size of the observation space\n",
    "    num_actions = env.action_space.n             # 3 actions. Boost Left, Right, Up\n",
    "    print(num_actions)\n",
    "    print(num_states)\n",
    "    \n",
    "    Q = np.zeros((num_states, num_actions))      # this is the Q table\n",
    "\n",
    "    for i in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        next_state = None\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) != epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                print(f\"Rand action: {action}\")\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "                print(f\"Q action: {action}\")\n",
    "\n",
    "            #(next_state, reward, done, info, glorp) = env.step(action)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            #done, info = next_state\n",
    "            #print(env.step(action))\n",
    "            # Update Q-value using Bellman equation\n",
    "            #Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "\n",
    "\n",
    "'''\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    print(action)\n",
    "    observation, reward, terminated, truncated, info = env.step(0)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "'''\n",
    "\n",
    "# no reward if y_accel is positive\n",
    "\n",
    "\n",
    "#q_learn(env, 100)\n",
    "learn_test(env, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5e415d3-1125-4d63-a358-1ee2d5957aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 1\n",
      "Episode: 2\n",
      "Episode: 3\n",
      "Episode: 4\n",
      "Episode: 5\n",
      "Episode: 6\n",
      "Episode: 7\n",
      "Episode: 8\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\",  render_mode=\"human\")\n",
    "#env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "\n",
    "def reward_zero(x, ep):\n",
    "\n",
    "    return 1 / (abs(x) + ep)\n",
    "\n",
    "def print_Q(Q):\n",
    "    print(\"      None     Left      Main      Right\")\n",
    "    print(f\"X      : {Q[0]}\")\n",
    "    print(f\"Y      : {Q[1]}\")\n",
    "    print(f\"X_accel: {Q[2]}\")\n",
    "    print(f\"Y_accel: {Q[3]}\")\n",
    "    print(f\"angle  : {Q[4]}\")\n",
    "    print(f\"leg l  : {Q[5]}\")\n",
    "    print(f\"leg r  : {Q[6]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def learn_test(env, epoc):\n",
    "    alpha = 0.01   # learning rate\n",
    "    gamma = 0.99  # discount factor, importance of future rewards\n",
    "    epsilon = 0.1 # how greedy the model should be \n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    num_episodes = 1000\n",
    "\n",
    "    good_Q = [\n",
    "             [ -0.02782456,   0.75230489,  -7.58057843,   0.20126472],\n",
    "             [ 16.04559171,  16.18043107,  16.89822651,  15.56941526],\n",
    "             [  9.77840283,  12.18235009,   9.80584967,  10.94169725],\n",
    "             [  0,           0,           0,           0,        ],\n",
    "             [ 10.41677421,   7.47216238,   4.81479032,   0.41579866],\n",
    "             [  4.17755791,   7.3522757,    4.03246393,   7.17187176],\n",
    "             [ 13.11819688,   2.22340608,  -0.85352378,   1.13964766],\n",
    "             [  0,         -21.10845444,   2.99062621,   6.56108225]\n",
    "                                                                    ]\n",
    "\n",
    "    num_states = env.observation_space.shape[0]  # the size of the observation space\n",
    "    num_actions = env.action_space.n             # 3 actions. Boost Left, Right, Up\n",
    "    #Q = np.zeros((num_states, num_actions))  \n",
    "    with open('q_table.pkl', 'rb') as f:\n",
    "        Q = pickle.load(f)\n",
    "    #Q = good_Q\n",
    "    \n",
    "    state, info = env.reset()\n",
    "\n",
    "    for i in range(epoc):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        next_state = None\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            #action = env.action_space.sample()\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(f\"Rand action: {action}\")\n",
    "            else:\n",
    "                action = np.argmax(Q[np.argmax(state)])\n",
    "                #print(f\"Q action: {action}\")\n",
    "        #(next_state, reward, done, info, glorp) = env.step(action)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            #print(f\"reward: {reward}\")\n",
    "\n",
    "            #x, y, x_velocity, y_velocity, angle = state[0], state[1], state[2], state[3], state[4]\n",
    "\n",
    "            '''\n",
    "            print(f\"x: {x}\")\n",
    "            print(f\"y: {y}\")\n",
    "            print(f\"x_velocity: {y_velocity}\")\n",
    "            print(f\"y_velocity: {y_velocity}\")\n",
    "            print(f\"angle: {angle}\")\n",
    "            '''\n",
    "            #print(f\"x_velocity: {y_velocity}\")\n",
    "\n",
    "            #reward += abs(reward_zero(state[2], 0.0001))\n",
    "\n",
    "            #print(f\"angle: {state[4]}\")\n",
    "            #print(f\"reward: {reward}\")\n",
    "\n",
    "            #bellman equation \n",
    "            old_val = Q[np.argmax(state),action]\n",
    "            next_max = np.max(Q[next_state])\n",
    "            new_val = old_val + alpha * (reward + gamma * next_max - old_val)\n",
    "            Q[np.argmax(state),action] = new_val\n",
    "\n",
    "            if epsilon > epsilon_min:\n",
    "                epsilon *= epsilon_decay\n",
    "\n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "\n",
    "            #done, info = next_state\n",
    "            #print(env.step(action))\n",
    "            # Update Q-value using Bellman equation\n",
    "            #Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "    env.close()\n",
    "    #with open('q_table.pkl', 'wb') as f:\n",
    "        #pickle.dump(Q, f)\n",
    "    #print_Q(Q)\n",
    "learn_test(env, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255f65a-2f5f-4dd2-b87d-258b7c51e806",
   "metadata": {},
   "source": [
    "### Explaining the code\n",
    "- Epsilon represents the %10 randomness that is added to force the AI to explore\n",
    "- Epsilon decay is so the AI becomes less random as time goes on and it becomes more confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de65fc8b-af09-4130-9f22-f3af1868024a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sleep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43msleep\u001b[49m(\u001b[38;5;241m100000\u001b[39m)\n\u001b[1;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sleep' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "sleep(100000)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a3b1e-1370-47fd-89b2-44fee4ea49e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
